# VerusSearch: Verification-Driven Agentic Search over Verus Proof States

Following completion of SFT, we will likely have a model that is strong at rote, formally-verifiable Rust programming. To improve the model's **process comprehension** (its ability to understand how edits change compilation/verification outcomes), we combine **search + verifier feedback + learning** to iteratively propose, rank, and apply patches until the code is both Rust-correct and formally proven against its specs. We’ll call this architecture **VerusSearch**.

## Optimization Goals
We have a number of optimization goals that transforms turns the production and editing of Rust code into a searchable verification problem, similar to approaches automated theorem proving. Goals 1 through 3 are primary goals, designed to produce hard, binary rewards for non-negotiable outcomes during Rust generation and Verus verification, while 4 through 6 are secondary goals that will ensure the model learns robustly and cannot game the system.

1. **Build succeeds** (`cargo build` / `rustc`)  
2. **Tests pass** (`cargo test`)  
3. **Verus verifies** (all targeted proof obligations discharged)
4. **No "cheating"**: don’t weaken specs, delete requirements, or use unsound escape hatches.
5. **Minimal, maintainable diffs**: fewer touched files, smaller patches, cleaner code.
6. **Search efficiency**: fewer tool invocations, faster wall-clock.

## Terms

- **Node / State**: a snapshot of the repo (files + metadata) plus tool feedback (compile/test/Verus outputs).
- **Action**: an edit to the repo (patch), e.g., add a loop invariant, change a function, add a spec, refactor.
- **Best-first search (BFS here)**: keep a **priority queue** of nodes; always expand the highest-scoring node next.
- **Expert iteration**: alternate (a) search to generate successful trajectories, (b) train a model on those trajectories, repeat [R-EXIT].
- **GRPO**: an online RL algorithm used in DeepSeek-R1 that estimates a baseline from a group of sampled outputs (no big critic model required) [R-GRPO].
- **DPO**: an offline preference-optimization method that trains a policy using pairwise "better vs worse" samples (no on-policy RL loop required) [R-DPO].
- **TCB (trusted computing base)**: must trust for "verified" to mean; for Verus it includes the specs, Verus, solvers, and Rust compiler [R-VERUS-TCB].

## VerusSearch's Proposed Architecture

### 1) I/O
**Input(s):**
- Natural language prompt containing instructions
- Existing repo or empty template
- Test harness (unit tests / property tests)
- Verification target list containing which modules/functions must verify

(all of these inputs wouldn't be passed by the user, and many can be autogenerated from just the prompt)

**Outputs:**
- Verified code changes + artifacts (logs, patches, proof summaries)

### 2) Evironment
Deterministic tools that grade a node:
- `cargo build` / `rustc` diagnostics
- `cargo test` results
- `verus` verification results (pass/fail, failing obligations, timeouts)

### 3) The Model(s)
We separate what to do next from how to grade it.

**Policy model (πθ)**: generates candidate patches.  
- We're starting small with an SFT'd **GPT-2** as the backbone for all sub-models that require an LLM.
- After derisking this architecture, we can eventually scale-up to DeepSeek-V3.2-Speciale.

**Value model (Vψ)**: predicts the probability a given node can reach a fully verified solution soon.  
- Not strictly required if node scoring is strong, but often improves search efficiency.

**Judge model**: checks spec non-regression, prompt alignment, and flags suspicious attempts, like writing a null Verus proof to avoid getting an error on a proof attempt. 

### 4) Search Controller
- Maintains frontier priority queue
- Chooses which node to expand next
- Chooses focus (which file(s)/function(s) to touch)
- Samples N candidate actions from πθ
- Evaluates each candidate with tools
- Logs everything (for training)

---

## State / Node Definition

A node `s` contains:

- `repo_snapshot`: git-like content-addressed store (can use `Jujutsu` or equivalent)
- `parent_id`, `patch`: how we got here
- `focus`: file/function being targeted
- `tool_result`:
  - `compile`: pass/fail, errors, warnings, time
  - `tests`: pass-rate, failures, time
  - `verus`: pass/fail, failing VCs summary, timeouts, time
- `features`: derived numbers for scoring 
- `score`: scalar for best-first ordering
- `signature`: hash of error types + locations for dedup/novelty

---

## Actions (Write vs Edit)

Unlike in theorem proving [R-ART], where our starting point is consistent across prompts (proof statement and sometimes a formalization of the proof statement), coding has far more in-between states, where search is heavily abbreviated since some of the code is already implemented, or existing part of the code have errors, and its far less tractable to start from scratch. VerusSearch supports **writing** and **editing** as action "modes":

### **Write**
Used when repo is empty or there’s no good starting implementation.
Actions:
- create new module(s)
- write full function bodies
- add specs (`requires/ensures`)
- add proof scaffolding (lemmas, invariants)

Search tends to be deeper and more exploratory.

### **Edit**
Used when there’s an existing baseline.
Actions:
- targeted patch to failing function
- add/repair a spec or loop invariant
- refactor a snippet into Verus-supported Rust
- localize change to minimal diff

Search can be much more surgical: prioritize nodes close to green.

## How we score nodes

As mentioned, we need:
1) **Hard constraints** (reject / huge penalty)
2) **Soft score** (scalar used by priority queue)
3) **Anti-cheating checks** (to prevent "trivial passes")

### A) Hard constraints
These rules prevent degenerate solutions:

- **Deletes required API** (function signatures removed/changed if contract says they must exist)
- **Disables tests or verification** (e.g., removes test targets, bypasses verus invocation)
- **Introduces known escape hatches** (project policy):
  - `assume`, `axiom`, `admit`, `unimplemented!()` in proof-critical paths
  - adds `unsafe` in ways that bypass intended safety/verification model
- **Spec regression (strict mode)**:
  - removing or weakening existing `ensures` / `requires` for target functions
  - replacing postconditions with `true`-like vacuity
- **Timeout abuse**:
  - changes designed to force verifier timeouts and treat them as passing (never treat timeouts as pass)

> We can tune strictness where in early tests we don't programmatically ban actions and instead allow them to incur massive penalties to more generally discourage the behavior during training before preventing it entirely, which hopefully will save tokens. 

### B) Soft sub-scores (all normalized to [0,1])
We compute these from tool output:

#### 1) Compile score `S_compile`
- If build succeeds: `1.0`
- Else: exponential penalty based on weighted error count:
  - weight errors > warnings
  - weight type errors > lint
  - weight "parser error" highest

Example:
- `err = Σ severity_weight(e)`
- `S_compile = exp(-k_c * err)` with `k_c ~ 0.25–0.5`

#### 2) Test score `S_test`
- If tests can’t run, `0`
- Else: `passed_tests / total_tests` (or better yet, a weighted version)
- Bonus for *new* tests that pass and increase coverage

#### 3) Verus score `S_verus`
We distinguish:
- **Fully verified**: `1.0`
- **Partial progress**: reward "fewer failing obligations" and "more verified functions" even if not done.

Let:
- `vc_fail` = weighted count of failing verification conditions
- `vc_timeout` = count of timeouts
- `verified_fn_ratio` = `verified_functions` / `target_functions`

Then:
- `S_verus` = `verified_fn_ratio` * `exp(-k_v * vc_fail)` * `exp(-k_t * vc_timeout)`

#### 4) Spec quality / anti-vacuity score `S_spec`
This is how we concretely prevent cheating and verify progress.

Components:
- **Coverage**: fraction of target functions with non-empty `requires`/`ensures`
- **Non-triviality heuristics**:
  - ensures mentions outputs + relevant inputs
  - avoids tautologies
- **Non-regression**:
  - compares normalized AST of specs to parent; penalize weakening
- **Dynamic sanity checks (optional but strong)**:
  - quick randomized execution tests for simple functions where executable
  - mutation testing: does the spec fail on obviously wrong outputs?

Compute:
- `S_spec` = `0.4*coverage` + `0.4*nontrivial` + `0.2*non_regress`
then clamp to [0,1]

SAFE uses an explicit spec filtering step (they note a quantitative metric to separate high/low quality specs) [R-SAFE]. We can borrow the principle even if we choose different concrete metrics.

#### 5) Patch size penalty `P_patch`
We want smaller diffs all else equal.
Let:
- `loc_changed`, `files_touched`
- `P_patch = clamp01( a*loc_changed + b*files_touched )` after normalization.

#### 6) Novelty / dedup penalty `P_dup`
If the node’s error signature matches many previous nodes, penalize:
- same `rustc` error code + same span
- same Verus VC failure type + same function

This prevents thrashing locally.

#### 7) Cost penalty `P_cost`
Penalty based on evaluation time:
- `P_cost` = `clamp01`(`time_seconds` / `budget_seconds`)

#### 8) Model confidence bonus `B_conf`
Encourage plausible edits without overfitting to the model:
- use avg logprob per generated token, length-normalized
- we can add style checker bonuses (formatting, idioms)

### C) Final scalar score and PQ priority
A workable default:

```
score(s) =
  4.0*S_verus
+ 2.0*S_compile
+ 1.5*S_test
+ 1.5*S_spec
+ 0.3*B_conf
- 1.0*P_patch
- 0.7*P_dup
- 0.7*P_cost
```

Then best-first priority is `priority = -score(s)`, so the higher score gets expanded first.

#### "Progress shaping" (important for long proofs)
Also compute a **delta bonus** vs parent:
- `Δ` = `score(s)` - `score(parent(s))`
- Add `+0.5*max(0,Δ)` to favor nodes that *actually improve*.

If the model output doesn’t reduce errors and/or increase verified coverage, it won’t climb the queue.

### D) Multi-queue diversification (avoids local minima)
Borrow BFS-Prover’s spirit of running multiple BFS configurations and aggregating successes [R-BFS]:
- Run searchers in parallel with slightly different weights:
  - min-diff searcher
  - max-verus-progress searcher
  - compile-first" searcher
  - spec-first searcher
- Union their best nodes into shared training logs.

This is a cheap way to get exploration without needing MCTS.

## Search Algorithm (Best-First Patch Search)

### Core loop
1. Initialize root node `s0` with baseline repo.
2. Push `s0` into priority queue.
3. Repeat until solved or budget exhausted:
   - Pop best node `s`
   - Choose a focus using heuristics:
     - where errors point
     - dependency graph (callers/callees)
   - Sample `N` candidate actions from πθ (temperature/top-p for diversity)
   - Apply each patch in a sandbox
   - Run tools → get `tool_result`
   - Score each child → push into PQ
   - Log (state, action, tool feedback, score)

### Works better than MCGS, exhaustive search methods
Node scoring is an informed heuristic combining deterministic feedback (compiler/verifier), learned priors (policy likelihood / optional value), and anti-gaming constraints, therefore the search is heavily guided to reduce errors and increase verified coverage. Then the model can learn heuristics that approximate which path will work:

1) **Value head / value model**
- Train `Vψ(s)` to predict probability of eventual success from node features.
- Use it as an extra additive term in node score.

2) **Focus selection model**
- Predict which file/function to edit next from diagnostic summaries.

3) **Macro-actions**
- Instead of one small patch, let the model emit:
  - a proof sketch (invariants/lemmas needed)
  - then specialized patch steps to implement it
This reduces branching factor.

4) **Retrieval-augmented tactics**
- Maintain a library of verified Verus idioms:
  - loop invariant templates
  - ghost state patterns
  - common lemmas
Retrieve the closest pattern before generating a patch.

## Learning Loop (Expert Iteration) 

### Search-driven data generation
Run VerusSearch with SFT πθ to produce:
- successful trajectories (full proof/edit paths)
- near-misses with rich error feedback

Store:
- `(state, action)` pairs along successful paths → for SFT
- `(state, good_action, bad_action)` preference tuples → for DPO

### Optimizing πθ

#### Option A: GRPO / PPO-style RL (online)
Use group sampling:
- For each training prompt/state, sample `G` candidate patches.
- Reward each candidate using **node score** (or terminal score).
- Normalize rewards within the group and update πθ.

GRPO is designed to be cheaper than PPO because it avoids a large critic by using group baselines [R-GRPO]. (Hsee ugging Face TRL’s GRPO trainer for practical reference [R-TRL-GRPO].)

**Add process-based rewards:**
Instead of "only final success", we can use **shaped rewards**:
- reward reductions in compile errors
- reward reductions in failing VCs
- reward spec non-regression
- final big reward for full verification

#### Option B: DPO refinement (offline; should start with this for derisking)
We can avoid the on-policy RL loop:

- For a given state `s`, generate multiple candidates `{a_i}`.
- Pick winner/loser pairs based on node score and tool feedback:
  - winner: fewer errors / more verified / non-regressing spec
  - loser: introduces errors, or regresses spec, or fails verification
- Train πθ with DPO on these pairs.

This directly mirrors BFS-Prover’s use of compiler-error negatives to improve tactic selection via DPO [R-BFS], and leverages the core DPO formulation [R-DPO].

### Curriculum & filtering (offline/online)
Like in BFS-Prover [R-BFS], we can:
- periodically run a cheap solver (greedy/beam)
- remove "easy" tasks from the training pool
- focus search/RL on the hard remainder

This prevents the dataset from collapsing into trivial fixes and improves generalization.

## Multi-file scaling for real-world use

### File targeting
For an unannotated codebase written in generic Rust, the model may not need to annotate every part of the codebase to generate and verify code or patches it produces. Instead, we can use a triage stack:
1. Parse diagnostics → map to file/function
2. Build lightweight dependency graph (callers/callees)
3. Rank targets by:
   - number/severity of errors
   - centrality (how many dependents)
   - expected payoff (e.g., a shared invariant fix)

Then:
- shard search across top-K targets (parallel workers)
- merge best nodes back into shared frontier

### Caching
- reuse `target/` incremental compilation
- memoize Verus results by file hash
- evaluate cheap checks first:
  - format, static lint, syntax
  - then compile
  - then tests
  - then Verus (most expensive)

## References
- [R-V32-HF] https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale
- [R-V32-PAPER] https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf?download=true
- [R-DS-PRICE] https://api-docs.deepseek.com/quick_start/pricing
- [R-EPOCH] https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
- [R-V3] https://arxiv.org/abs/2412.19437
- [R-GRPO] https://arxiv.org/pdf/2501.12948
- [R-TRL-GRPO] https://huggingface.co/docs/trl/main/en/grpo_trainer
- [R-DPO] https://arxiv.org/abs/2305.18290
- [R-BFS] https://aclanthology.org/2025.acl-long.1565.pdf
- [R-SAFE] https://proceedings.iclr.cc/paper_files/paper/2025/file/b2e20d7402c9985eae4ba924c65370a8-Paper-Conference.pdf
- [R-VERUS-TCB] https://www.cs.utexas.edu/~hleblanc/pdfs/verus.pdf
- [R-DS-CODER] https://github.com/deepseek-ai/DeepSeek-Coder
- [R-DS-CODERV2] https://github.com/deepseek-ai/DeepSeek-Coder-V2
- [R-EXIT] https://arxiv.org/abs/1705.08439
- [R-PBRS] https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf
- [R-ART] https://arxiv.org/html/2510.01346v1
