**4,496 tasks** generated from verified Rust code with formal specifications.

**Location:** `workspace/verus_dataset.sqlite3`
**Exports:** `workspace/exports/*.jsonl`
```
workspace/
├── verus_dataset.sqlite3          # Main database
└── exports/
    ├── all_tasks.jsonl            # All 4,496 tasks (11.2 MB)
    ├── train.jsonl                # 4,047 tasks (90%)
    ├── val.jsonl                  # 449 tasks (10%)
    ├── spec_gen.jsonl             # 1,625 tasks
    ├── code_synth.jsonl           # 1,234 tasks
    └── spec_and_code.jsonl        # 1,637 tasks
```

**JSONL Format:**
```json
{
  "task_uid": "abc123...",
  "task_type": "spec_gen",
  "prompt": "Add Verus verification specifications...",
  "target": "fn foo() requires ... { ... }",
  "text": "<prompt + target combined for SFT>",
  "meta": {
    "function_name": "foo",
    "source_repo": "vericoding-benchmark",
    "sample_uid": "xyz789..."
  }
}
```

**Training usage snippet**:
```python
import json

with open("workspace/exports/train.jsonl") as f:
    train_data = [json.loads(line) for line in f]

for entry in train_data[:3]:
    print(f"Task: {entry['task_type']}")
    print(f"Function: {entry['meta']['function_name']}")
    print(f"Prompt length: {len(entry['prompt'])}")
```
---
## Construction

The dataset was constructed by crawling four Verus repositories: `vericoding-benchmark` (LLM-generated solutions), `vostd` (verified OS standard library), `verus-lang-verus` (official examples), and `verified-storage` (persistent memory systems). Each repository was cloned locally and scanned for `.rs` files containing Verus constructs. The scanner identified 4,309 source files, which were indexed in the `source_files` table with metadata including path, hash, line count, and a heuristic "verus score" indicating density of verification constructs.

From these source files, code samples were extracted and verified. Verification was performed differently per repository: `vericoding-benchmark` and `verus-lang-verus` used standalone Verus invocations on individual files, while `vostd` required a custom toolchain built via `cargo xtask bootstrap` followed by `make verify`. ==The `verified-storage` repository was partially verified as only the `pmemlog` crate succeeded on macOS; other crates (`multilog`, `capybaraKV`, `verismo`) failed due to Linux-specific dependencies and were excluded. All 1,219 samples in the final dataset have `verification_result = 'pass'`, meaning Verus confirmed their correctness.==

Task generation then parsed each verified sample to extract executable functions with specifications. The parser identifies function boundaries, detects `requires`/`ensures`/`decreases` clauses, and filters out stub implementations containing `assume(false)` or `unimplemented!()` placeholders. For each qualifying function, up to three task types are generated by programmatically transforming the source: removing specs for `spec_gen`, stubbing the body for `code_synth`, or both for `spec_and_code`.

---
## Structure

The SQLite database (`workspace/verus_dataset.sqlite3`) contains eight tables. The core pipeline uses three: `repos` (6 rows) stores repository metadata; `samples` (1,219 rows) stores verified code snippets with foreign keys to their source repo and file; `tasks` (4,496 rows) stores the generated training examples with foreign keys to their source sample.

The `source_files` table (4,309 rows) indexes all scanned Rust files before filtering. The `verify_runs` and `reduction_jobs` tables are empty (I left them in case we can construct a more granular pipeline that logs individual verification commands and code minimization steps, but the bulk verification approach bypassed this since aggressive minimization didn't sufficiently contribute much to the dataset, see [See why minimization wasn't used](#aggressive-minimization-failure-modes)). The `meta` table holds schema versioning (`schema_version: 1.0.0`).

==The `file_id` column in `samples` is nullable because 7 samples were created programmatically during bulk imports (particularly from `vostd`) without associating them to a specific indexed source file.== The remaining 1,212 samples link back to their origin file, enabling traceability from task to sample to file to repository.

---
## Task Types

**`spec_gen`** (1,625 tasks): The model receives a complete function implementation without specifications and must add appropriate `requires`/`ensures` clauses. This teaches the model to infer preconditions and postconditions from code behavior.

```rust
// PROMPT: function with body, no specs
fn vec_clone(c: &Vec<f32>) -> (result: Vec<f32>)
{
    let mut result: Vec<f32> = Vec::new();
    let mut i: usize = 0;
    while i < c.len()
        invariant
            i <= c.len(),
            result.len() == i,
            forall|k: int| 0 <= k < (i as int) ==> result@[k] == c@[k],
        decreases c.len() - i
    {
        result.push(c[i]);
        i = i + 1;
    }
    result
}

// TARGET: same function with ensures clause added
fn vec_clone(c: &Vec<f32>) -> (result: Vec<f32>)
    ensures
        result.len() == c.len(),
        result.view() == c.view(),
{ /* same body */ }
```

**`code_synth`** (1,234 tasks): The model receives a function signature with specifications but a stubbed body (`unimplemented!()`) and must synthesize an implementation that satisfies the spec. This teaches verified code generation from formal specifications.

```rust
// PROMPT: specs present, body stubbed
fn zap_negatives(a: &mut Vec<i32>)
    ensures
        a.len() == old(a).len(),
        forall|i: int| 0 <= i < a.len() ==>
            if old(a)[i] < 0 { a[i] == 0 } else { a[i] == old(a)[i] }
{
    unimplemented!()
}

// TARGET: real implementation satisfying the spec
fn zap_negatives(a: &mut Vec<i32>)
    ensures /* same */
{
    let mut i = 0;
    while i < a.len() { if a[i] < 0 { a.set(i, 0); } i += 1; }
}
```

**`spec_and_code`** (1,637 tasks): The model receives only the function signature with a stubbed body and must produce both specifications and implementation. This is the hardest task, requiring the model to understand intent from the signature alone.

```rust
// PROMPT: signature only, no specs, no body
pub fn new(slot: &MetaSlot) -> (res: (Page<M>, Tracked<PageModel>)) {
    unimplemented!()
}

// TARGET: complete function with specs and implementation
pub fn new(slot: &MetaSlot) -> (res: (Page<M>, Tracked<PageModel>))
    ensures
        res.0.relate_meta_slot(slot),
        res.1@.relate_meta_slot(slot),
        res.0.relate_model(&res.1@),
{
    let page = Page::from_slot(slot);
    let Tracked(model) = Page::<M>::model_from_slot(slot);
    (page, Tracked(model))
}
```

---
## Distribution

Samples vary significantly in complexity. The `vericoding-benchmark` contributes 929 samples averaging 62 lines of short, focused functions ideal for learning basic patterns. The `vostd` library provides 168 samples averaging 281 lines of production-grade verified systems code. The `verified-storage` samples are the most complex at 427 lines average, representing real persistent memory verification. This distribution ensures the model sees both simple learning examples and complex real-world code.

---
## Split Strategy

The dataset is exported to JSONL with a 90/10 train/validation split (4,047 train, 449 val) using a fixed random seed (42) for reproducibility. ==The split is performed at the task level after shuffling, meaning tasks from the same sample may appear in both splits==, which is intentional, as the tasks test different skills (spec generation vs code synthesis) even for the same underlying function. Task-type-specific exports (`spec_gen.jsonl`, `code_synth.jsonl`, `spec_and_code.jsonl`) are included for curriculum learning or task-specific fine-tuning if needed.

---
## Training Utility

Each JSONL entry contains `prompt` (model input), `target` (expected output), and `text` (concatenated for standard SFT). The `meta` field includes `function_name`, `source_repo`, and `sample_uid` for analysis and debugging. During training, the three task types can be mixed for general Verus competency, or separated for staged curriculum learning, starting with `spec_gen` (easiest, specs from code), progressing to `code_synth` (medium, code from specs), and finally `spec_and_code` (hardest, both from signature). The verified ground truth ensures that all targets actually pass Verus, providing a reliable signal for learning correct formal verification patterns.

---
## Aggressive Minimization Failure Modes

Tried `creduce` to minimize samples; shrinking verified code while preserving verification success, but as mentioned, for the final dataset this approach was abandoned for a few reasons:
- **C-Reduce produces unusable output for training data.** C-Reduce is designed to minimize bug-reproducing test cases, not to preserve meaningful code structure. When applied to a 277-line Verus sample with an interestingness test requiring verification success, C-Reduce reduced it to `use vstd;`. This technically "verifies" (0 functions, 0 errors) but contains no training signal. Even after modifying the interestingness test to require at least one verified function, C-Reduce produced malformed code:
  ```rust

  fn d(a: Vec<int>) -> Vec<int> requires { Vec::new() }

  fn f(a: Vec<int>, limit: usize, x: int) -> usize requires { let mut g= 0; ... }

  ```
  The specs are empty or garbled, formatting is destroyed, and function names are reduced to single letters, defeating the purpose of the training data.
- **Most samples cannot verify standalone.** The `vostd` and `verified-storage` samples use internal imports (`use crate::...`, `use super::...`) and require their full crate build context. Standalone Verus invocation fails immediately. Only `vericoding-benchmark` samples (which are already minimal at 62 lines average) can verify independently.
- **Function-level extraction was also attempted.** As a fallback, individual functions with specs were extracted from `verus-lang-verus` examples into standalone files. This produced 41 new samples, but many were duplicates or trivial (e.g., `proof fn check_eq(x: Seq<int>, y: Seq<int>) requires x == y {}`). The net gain was ~50 tasks (~1% increase), which was not worth the complexity.
- **The existing task generation is already "minimized" in the right sense.** Each task extracts a single function from a sample and presents it with focused prompt/target pairs. The model sees the function in isolation, which is the granularity that matters for learning, so sample-level minimization would only remove surrounding context that the task generation already filters out.
